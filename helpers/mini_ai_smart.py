import pandas as pd
import os, re
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class MiniLegalAI:
    def __init__(self, workbook_path=None):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ ÙˆØ±Ø¨Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."""
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† config
        config = st.session_state.get("config", {})
        self.ai_enabled = config.get("AI", {}).get("ENABLE", True)
        if not self.ai_enabled:
            st.warning("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹Ø·Ù„ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù….")
            return

        self.workbook_path = workbook_path or config.get("WORKBOOK_PATH", "AlyWork_Law_Pro_v2025_v24_ColabStreamlitReady.xlsx")
        self.memory_path = config.get("AI", {}).get("MEMORY_PATH", "ai_memory.json")
        self.logs_path = config.get("AI", {}).get("LOGS_PATH", "AI_Analysis_Logs.csv")
        self.max_history = config.get("AI", {}).get("MAX_HISTORY", 20)

        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„Ù…ØµÙÙˆÙØ©
        self.db = self.load_database()
        self.vectorizer = None
        self.tfidf_matrix = None
        self.build_tfidf_matrix()

    # ==============================
    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    # ==============================
    @st.cache_data(show_spinner=False, allow_output_mutation=True)
    def load_database(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."""
        if not os.path.exists(self.workbook_path):
            st.warning(f"âš ï¸ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {self.workbook_path}")
            return pd.DataFrame(columns=['Ø§Ù„Ù…Ø§Ø¯Ø©', 'Ø§Ù„Ù‚Ø³Ù…', 'Ø§Ù„Ù†Øµ', 'Ù…Ø«Ø§Ù„'])
        try:
            with st.spinner("â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©..."):
                df = pd.read_excel(self.workbook_path, engine='openpyxl')
                df.fillna("", inplace=True)
                return df
        except Exception as e:
            st.error(f"âš ï¸ Ø®Ø·Ø£ Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return pd.DataFrame(columns=['Ø§Ù„Ù…Ø§Ø¯Ø©', 'Ø§Ù„Ù‚Ø³Ù…', 'Ø§Ù„Ù†Øµ', 'Ù…Ø«Ø§Ù„'])

    # ==============================
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
    # ==============================
    def preprocess_text(self, text):
        text = str(text).strip()
        text = re.sub(r"[^\w\s]", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text

    # ==============================
    # Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF
    # ==============================
    @st.cache_data(show_spinner=False, allow_output_mutation=True)
    def build_tfidf_matrix(self):
        """Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."""
        if self.db.empty:
            return
        text_col = next((c for c in self.db.columns if "Ù†Øµ" in c), None)
        if not text_col:
            st.error("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ù„Ù.")
            return
        corpus = self.db[text_col].apply(self.preprocess_text).tolist()
        self.vectorizer = TfidfVectorizer()
        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)

    # ==============================
    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ
    # ==============================
    def advanced_search(self, query, top_n=1):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ÙƒØ«Ø± ØªØ·Ø§Ø¨Ù‚Ù‹Ø§ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…."""
        if not self.ai_enabled:
            return "ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹Ø·Ù„.", "", ""

        if self.db.empty or self.tfidf_matrix is None:
            return "âš ï¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ©.", "", ""

        query_clean = self.preprocess_text(query)
        query_vec = self.vectorizer.transform([query_clean])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
        top_indices = similarities.argsort()[::-1][:top_n]

        if similarities[top_indices[0]] == 0:
            return "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ Ù…Ø¨Ø§Ø´Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.", "", ""

        results = []
        for i in top_indices:
            row = self.db.iloc[i]
            score = round(similarities[i] * 100, 2)
            results.append({
                "Ø§Ù„Ù†Øµ": row.get("Ø§Ù„Ù†Øµ", ""),
                "Ø§Ù„Ù…Ø§Ø¯Ø©": row.get("Ø§Ù„Ù…Ø§Ø¯Ø©", ""),
                "Ø§Ù„Ù‚Ø³Ù…": row.get("Ø§Ù„Ù‚Ø³Ù…", ""),
                "Ù…Ø«Ø§Ù„": row.get("Ù…Ø«Ø§Ù„", ""),
                "Ø§Ù„ØªØ·Ø§Ø¨Ù‚": f"{score}%"
            })

        best = results[0]
        return best["Ø§Ù„Ù†Øµ"], f"Ø§Ù„Ù…Ø§Ø¯Ø© {best['Ø§Ù„Ù…Ø§Ø¯Ø©']} - Ø§Ù„Ù‚Ø³Ù…: {best['Ø§Ù„Ù‚Ø³Ù…']} (Ø¯Ù‚Ø© {best['Ø§Ù„ØªØ·Ø§Ø¨Ù‚']})", best["Ù…Ø«Ø§Ù„"]

    # ==============================
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©
    # ==============================
    def reload(self, new_path=None):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ø§Ù„ØªØ­Ø¯ÙŠØ«."""
        if new_path:
            self.workbook_path = new_path
        self.db = self.load_database()
        self.build_tfidf_matrix()
        st.success("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")