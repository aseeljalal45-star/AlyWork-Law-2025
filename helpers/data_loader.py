import pandas as pd
import streamlit as st
import os
import requests
from typing import Union, Optional, Dict, Any
from datetime import datetime
import tempfile

# ==============================
# ğŸ“‚ Ù…Ø¯ÙŠØ± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# ==============================
class DataLoader:
    """
    ğŸš€ Ù…Ø¯ÙŠØ± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù†
    
    ÙŠØ¯Ø¹Ù… Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ù…Ø±ÙˆÙ†Ø©
    """
    
    def __init__(self):
        self.load_history = []
        self.supported_formats = ['.csv', '.xlsx', '.xls', '.json']
        self.max_file_size_mb = 50
        
    @st.cache_data(ttl=600, show_spinner="Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    def load_data(_self, source_path: str, **kwargs) -> pd.DataFrame:
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡
        
        Args:
            source_path (str): Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø·
            **kwargs: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ù…Ù„Ù
            
        Returns:
            pd.DataFrame: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø© Ø£Ùˆ DataFrame ÙØ§Ø±Øº
        """
        try:
            # ØªØ³Ø¬ÙŠÙ„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
            _self._log_loading_attempt(source_path)
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø± ÙˆØªÙˆØ¬ÙŠÙ‡ Ù„Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            if _self._is_google_sheets_url(source_path):
                df = _self._load_google_sheets(source_path, **kwargs)
            elif _self._is_http_url(source_path):
                df = _self._load_http_data(source_path, **kwargs)
            elif source_path.endswith('.xlsx') or source_path.endswith('.xls'):
                df = _self._load_excel_file(source_path, **kwargs)
            elif source_path.endswith('.csv'):
                df = _self._load_csv_file(source_path, **kwargs)
            elif source_path.endswith('.json'):
                df = _self._load_json_file(source_path, **kwargs)
            else:
                raise ValueError(f"âŒ ØµÙŠØºØ© Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {source_path}")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„
            if not df.empty:
                df = _self._post_process_data(df, source_path)
                _self._log_successful_load(source_path, len(df))
                
            return df
            
        except Exception as e:
            _self._log_loading_error(source_path, str(e))
            return pd.DataFrame()

    def _is_google_sheets_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Google Sheets"""
        return "docs.google.com/spreadsheets" in url

    def _is_http_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ø©"""
        return url.startswith(('http://', 'https://'))

    def _load_google_sheets(self, url: str, **kwargs) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Sheets"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¹Ø±Ø¶ Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· ØªØµØ¯ÙŠØ± CSV
            if "/edit" in url:
                url = url.replace("/edit", "/export?format=csv")
            elif "export?format=csv" not in url:
                sheet_id = self._extract_google_sheets_id(url)
                if sheet_id:
                    url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv"
            
            df = pd.read_csv(url, **kwargs)
            st.success(f"ğŸ“Š ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} ØµÙ Ù…Ù† Google Sheets")
            return df
            
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Google Sheets: {e}")
            return pd.DataFrame()

    def _extract_google_sheets_id(self, url: str) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Google Sheets Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        import re
        pattern = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'
        match = re.search(pattern, url)
        return match.group(1) if match else None

    def _load_http_data(self, url: str, **kwargs) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø±ÙˆØ§Ø¨Ø· HTTP Ø¹Ø§Ù…Ø©"""
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙˆØ¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø£Ùˆ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
            if url.endswith('.csv'):
                import io
                df = pd.read_csv(io.StringIO(response.text), **kwargs)
            elif url.endswith('.json'):
                df = pd.read_json(io.StringIO(response.text), **kwargs)
            else:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ CSV Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§
                import io
                df = pd.read_csv(io.StringIO(response.text), **kwargs)
                
            st.success(f"ğŸŒ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} ØµÙ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·")
            return df
            
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·: {e}")
            return pd.DataFrame()

    def _load_excel_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù Excel"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
        
        # ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        if file_size_mb > self.max_file_size_mb:
            st.warning(f"âš ï¸ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ÙƒØ¨ÙŠØ± ({file_size_mb:.1f} MB). Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆÙ‚ØªÙ‹Ø§ Ø£Ø·ÙˆÙ„.")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        engines = ['openpyxl', 'xlrd']
        for engine in engines:
            try:
                df = pd.read_excel(file_path, engine=engine, **kwargs)
                st.success(f"ğŸ“— ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} ØµÙ Ù…Ù† Ù…Ù„Ù Excel")
                return df
            except Exception as e:
                continue
        
        raise Exception("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Excel Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©")

    def _load_csv_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù CSV"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª CSV Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        default_kwargs = {
            'encoding': 'utf-8',
            'sep': ',',
            'skipinitialspace': True
        }
        default_kwargs.update(kwargs)
        
        try:
            df = pd.read_csv(file_path, **default_kwargs)
            st.success(f"ğŸ“„ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} ØµÙ Ù…Ù† Ù…Ù„Ù CSV")
            return df
        except UnicodeDecodeError:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨ØªØ´ÙÙŠØ±Ø§Øª Ø£Ø®Ø±Ù‰
            for encoding in ['latin1', 'iso-8859-1', 'cp1252']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, **kwargs)
                    st.success(f"ğŸ“„ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} ØµÙ Ù…Ù† Ù…Ù„Ù CSV (Ø¨ØªØ±Ù…ÙŠØ² {encoding})")
                    return df
                except UnicodeDecodeError:
                    continue
            raise Exception("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ´ÙÙŠØ±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©")

    def _load_json_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù JSON"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
        
        try:
            df = pd.read_json(file_path, **kwargs)
            st.success(f"ğŸ“‹ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} ØµÙ Ù…Ù† Ù…Ù„Ù JSON")
            return df
        except Exception as e:
            raise Exception(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON: {e}")

    def _post_process_data(self, df: pd.DataFrame, source_path: str) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ© ØªÙ…Ø§Ù…Ù‹Ø§
        initial_count = len(df)
        df = df.dropna(how='all')
        
        if len(df) < initial_count:
            st.info(f"ğŸ§¹ ØªÙ… Ø¥Ø²Ø§Ù„Ø© {initial_count - len(df)} ØµÙ ÙØ§Ø±Øº")
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙÙ‡Ø±Ø³
        df = df.reset_index(drop=True)
        
        # ØªØ­ÙˆÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¥Ù„Ù‰ Ù†Øµ ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø£ÙŠ Ù‚ÙŠÙ… ØºÙŠØ± Ù†ØµÙŠØ©
        df.columns = df.columns.astype(str)
        
        # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ©
        text_columns = df.select_dtypes(include=['object']).columns
        df[text_columns] = df[text_columns].fillna('')
        
        return df

    def _log_loading_attempt(self, source_path: str):
        """ØªØ³Ø¬ÙŠÙ„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        self.load_history.append({
            'timestamp': datetime.now().isoformat(),
            'source': source_path,
            'type': 'attempt',
            'status': 'started'
        })

    def _log_successful_load(self, source_path: str, row_count: int):
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø§Ø¬Ø­"""
        self.load_history.append({
            'timestamp': datetime.now().isoformat(),
            'source': source_path,
            'type': 'success',
            'status': 'completed',
            'row_count': row_count
        })

    def _log_loading_error(self, source_path: str, error_msg: str):
        """ØªØ³Ø¬ÙŠÙ„ Ø®Ø·Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        self.load_history.append({
            'timestamp': datetime.now().isoformat(),
            'source': source_path,
            'type': 'error',
            'status': 'failed',
            'error': error_msg
        })
        st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {source_path}")

    def get_load_history(self, last_n: int = 10) -> list:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        return self.load_history[-last_n:]

    def clear_cache(self):
        """Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        st.cache_data.clear()
        st.success("ğŸ—‘ï¸ ØªÙ… Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

# ==============================
# ğŸ¯ Ø¯ÙˆØ§Ù„ Ø³Ø±ÙŠØ¹Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
# ==============================
@st.cache_data(ttl=600)
def load_data_smart(source_path: str, **kwargs) -> pd.DataFrame:
    """
    Ø¯Ø§Ù„Ø© Ø³Ø±ÙŠØ¹Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ)
    
    Args:
        source_path (str): Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø·
        **kwargs: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        
    Returns:
        pd.DataFrame: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©
    """
    loader = DataLoader()
    return loader.load_data(source_path, **kwargs)

def get_data_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©
    
    Args:
        df (pd.DataFrame): Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
    Returns:
        dict: Ù…Ù„Ø®Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    if df.empty:
        return {"status": "empty", "message": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"}
    
    return {
        "status": "loaded",
        "row_count": len(df),
        "column_count": len(df.columns),
        "columns": df.columns.tolist(),
        "data_types": df.dtypes.astype(str).to_dict(),
        "memory_usage_mb": df.memory_usage(deep=True).sum() / (1024 * 1024),
        "sample_data": df.head(3).to_dict('records')
    }

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¯ÙˆØ§Ù„
    st.title("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ù…Ù„Ù Ù…Ø«Ø§Ù„
    test_path = "example_data.csv"
    df = load_data_smart(test_path)
    
    if not df.empty:
        summary = get_data_summary(df)
        st.json(summary)